import numpy
from keras.models import Sequential
from keras.layers import Dense
from sklearn.model_selection import StratifiedKFold
# from keras.layers import Flatten
# from keras.layers.embeddings import Embedding
# from keras.layers import Dropout
# from keras.layers import LSTM
# from keras.layers import Conv1D, GlobalMaxPooling1D
# from keras.layers import Activation

# DATASET_FILENAME = "Dataset\Personality Prediction\Arff\Fb features\dataset_fb_features_openness.csv"
DATASET_FILENAME = "Dataset\Personality Prediction\Arff\Splice features\dataset_splice_openness.csv"
# DATASET_FILENAME = "Dataset\Personality Prediction\Arff\LIWC features\dataset_liwc_openness.csv"
# DATASET_FILENAME = "Dataset\Personality Prediction\Arff\All features\dataset_openness.csv"
# DATASET_FILENAME = "Dataset\Personality Prediction\Arff\dataset_openness.csv" #LIWC & Splice
# DATASET_FILENAME = "Dataset\pima-indians-diabetes.csv"

seed = 3
numpy.random.seed(seed)
DELIMITER = ","
total_features = 74
index_class = total_features
epochs = 100
batchSize = 25

print ("Loading dataset")
dataset = numpy.loadtxt(DATASET_FILENAME, delimiter=DELIMITER)
data = dataset[:,0:total_features]
labels = dataset[:,index_class]

# datasetPredict = numpy.loadtxt("Dataset\pima-indians-diabetess.csv", delimiter=DELIMITER)
# predictData = datasetPredict[:,0:total_features]

print("Cross Validation")
kFold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)
cvValues = []
kFoldIdx = 1
for train, test in kFold.split(data, labels):
    print ("K-Fold: " + str(kFoldIdx))
    kFoldIdx += 1
    
    x_train = data[train]
    y_train = labels[train]
    x_test = data[test]
    y_test = labels[test]
    
    #Create model
#     print ("Creating model")
    model = Sequential()
    model.add(Dense(125, init='uniform', input_dim=total_features, activation='relu'))
    model.add(Dense(25, init='uniform', activation='relu'))
    model.add(Dense(5, init='uniform', activation='relu'))
    model.add(Dense(1, activation='sigmoid'))
    
#     print ("Compiling model")
    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
    
#     print ("Training model")
    cvValues.append(model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=epochs, batch_size=batchSize, verbose=2))
    
#     print("\n\nEvaluating model")
#     scores = model.evaluate(X, Y, batch_size=25, verbose=2)
#     print("%s: %f%%" % (model.metrics_names[0], scores[0]))
#     print("%s: %.2f%%" % (model.metrics_names[1], scores[1]*100))
#     predictions = model.predict(X)
#     rounded = [round(x[0]) for x in predictions]
#     print("\nPredictions result")
#     print(rounded)

cvLoss = []
cvAcc = []
for value in cvValues:
    cvLoss.append(value.history['val_loss'][epochs-1])
    cvAcc.append(value.history['val_acc'][epochs-1])
print("Validation Average Loss: %.2f%% (+/- %.2f%%)" % (numpy.mean(cvLoss), numpy.std(cvLoss)))
print("Validation Average Accuracy: %.2f%% (+/- %.2f%%)" % (numpy.mean(cvAcc), numpy.std(cvAcc)))